---
phase: 23-remove-latex2sympy
plan: 01
type: tdd
---

<objective>
Build custom LaTeX expression tokenizer that correctly identifies variables, units, operators, and numbers.

Purpose: Create the foundation for replacing latex2sympy by implementing a tokenizer with priority-ordered pattern matching that won't split multi-letter identifiers.
Output: Working `expression_tokenizer.py` module with comprehensive tests.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-phase.md
@~/.claude/get-shit-done/templates/summary.md
@~/.claude/get-shit-done/references/tdd.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/23-remove-latex2sympy/23-RESEARCH.md

# Relevant source files:
@src/livemathtex/parser/calculation_parser.py (Span class to reuse)
@tests/test_calculation_parser.py (test pattern reference)

**Branch:** `feature/v3-pure-pint` (development branch for v3.0)

**Tech stack available:**
- Pint (unit handling)
- pylatexenc (LaTeX entity handling)
- pytest (testing)

**Established patterns:**
- Span dataclass for position tracking
- TokenType enum pattern from token_classifier.py

**Key constraint from research:**
- Pattern priority ordering is CRITICAL: units before single letters
- `kg` must tokenize as UNIT, not `k`, `g` variables
- `\text{MWh}` and `\mathrm{kg}` must be captured as UNIT tokens
</context>

<feature>
  <name>LaTeX Expression Tokenizer</name>
  <files>src/livemathtex/parser/expression_tokenizer.py, tests/test_expression_tokenizer.py</files>
  <behavior>
    Tokenize LaTeX math expressions into typed tokens.

    Cases:
    - `3.14` → [NUMBER("3.14")]
    - `1e-6` → [NUMBER("1e-6")]
    - `E_{26}` → [VARIABLE("E_{26}")]
    - `PPE_{eff}` → [VARIABLE("PPE_{eff}")]
    - `R^2` → [VARIABLE("R^2")]
    - `\text{kg}` → [UNIT("kg")]
    - `\mathrm{MWh}` → [UNIT("MWh")]
    - `10\ \text{kg}` → [NUMBER("10"), UNIT("kg")]
    - `a + b` → [VARIABLE("a"), OPERATOR("+"), VARIABLE("b")]
    - `x \cdot y` → [VARIABLE("x"), OPERATOR("\cdot"), VARIABLE("y")]
    - `\frac{a}{b}` → [FRAC, LBRACE, VARIABLE("a"), RBRACE, LBRACE, VARIABLE("b"), RBRACE]
    - `(x + y)` → [LPAREN, VARIABLE("x"), OPERATOR("+"), VARIABLE("y"), RPAREN]
    - `\alpha` → [VARIABLE("\alpha")]
    - `\mu mol` → [VARIABLE("\mu"), VARIABLE("mol")] (Greek letter + variable)

    Edge cases:
    - Whitespace (\ , spaces) → skipped
    - Line breaks (\\) → skipped
    - Unknown characters → skipped with warning
  </behavior>
  <implementation>
    Based on research code skeleton in 23-RESEARCH.md:

    1. Create TokenType enum with: NUMBER, VARIABLE, UNIT, OPERATOR, FRAC, LPAREN, RPAREN, LBRACE, RBRACE, EOF

    2. Create Token dataclass with: type, value, start, end

    3. Create ExpressionTokenizer class with:
       - Priority-ordered PATTERNS list (units FIRST, single letters LAST)
       - `tokenize()` method returning List[Token]
       - `_next_token()` for greedy pattern matching

    4. Pattern priority (from research):
       a. Units in \text{}/\mathrm{} - HIGHEST
       b. Numbers (including scientific notation)
       c. Variables with subscripts/superscripts
       d. Greek letters
       e. Fraction command
       f. Operators (\cdot, \times, +, -, *, /, ^)
       g. Parentheses
       h. Braces
       i. Single letters - LOWEST (fallback)
       j. Whitespace - skip (None return)

    5. Use Span from calculation_parser.py or define compatible start/end
  </implementation>
</feature>

<verification>
```bash
# Run tokenizer tests
cd /home/mark/Repositories/livemathtex
pytest tests/test_expression_tokenizer.py -v

# Verify no regressions
pytest --tb=short

# Test count should increase from 365+
pytest --collect-only | tail -5
```
</verification>

<success_criteria>
- Failing tests written for all token types (RED)
- ExpressionTokenizer passes all tests (GREEN)
- Code cleaned up if needed (REFACTOR)
- All 3 TDD commits present
- All existing 365+ tests still pass
- New tests in test_expression_tokenizer.py
</success_criteria>

<output>
After completion, create `.planning/phases/23-remove-latex2sympy/23-01-SUMMARY.md`:

# Phase 23 Plan 01: Expression Tokenizer Summary

**[What shipped]**

## Accomplishments
- RED: What tests were written, why they failed
- GREEN: What implementation made them pass
- REFACTOR: What cleanup was done (if any)

## Files Created/Modified
- `src/livemathtex/parser/expression_tokenizer.py` - New tokenizer module
- `tests/test_expression_tokenizer.py` - Tokenizer tests

## Commits
- test(23-01): add failing tests for expression tokenizer
- feat(23-01): implement LaTeX expression tokenizer
- refactor(23-01): [if any cleanup]

## Decisions Made
[Key decisions and rationale]

## Issues Encountered
[Problems and resolutions]

## Next Step
Ready for Phase 24 (Expression Parser) or 23-02 if additional plans needed
</output>
