# Plan 37-01: Tokenizer Extension for Array Syntax

## Objective

Add bracket token types to support array literal syntax `[1, 2, 3]` and index access `arr[0]`.

## Context

The expression tokenizer currently recognizes:
- Numbers, variables, units
- Operators: `+`, `-`, `*`, `/`, `^`, `,`
- Parentheses: `(`, `)`
- Braces: `{`, `}`
- LaTeX commands: `\frac`, `\sqrt`, `\ln`, etc.

Need to add:
- Square brackets: `[`, `]`

Note: Comma is already recognized as an operator (added for function calls in Phase 34).

## Files to Modify

1. `src/livemathtex/parser/expression_tokenizer.py`

## Changes

### 1. Add TokenType entries

```python
class TokenType(Enum):
    # ... existing types ...
    LBRACKET = "lbracket"  # [
    RBRACKET = "rbracket"  # ]
```

### 2. Add patterns

Add to PATTERNS list after parentheses, before single-letter fallback:

```python
# Square brackets (for arrays)
(re.compile(r"\["), TokenType.LBRACKET, False),
(re.compile(r"\]"), TokenType.RBRACKET, False),
```

## Tests

Create `tests/test_expression_tokenizer_arrays.py`:

```python
def test_tokenize_array_literal():
    tokens = ExpressionTokenizer("[1, 2, 3]").tokenize()
    assert tokens[0].type == TokenType.LBRACKET
    assert tokens[1].type == TokenType.NUMBER
    assert tokens[2].type == TokenType.OPERATOR  # comma
    assert tokens[3].type == TokenType.NUMBER
    # ... etc
    assert tokens[-2].type == TokenType.RBRACKET

def test_tokenize_array_access():
    tokens = ExpressionTokenizer("arr[0]").tokenize()
    assert tokens[0].type == TokenType.VARIABLE
    assert tokens[1].type == TokenType.LBRACKET
    assert tokens[2].type == TokenType.NUMBER
    assert tokens[3].type == TokenType.RBRACKET
```

## Verification

```bash
pytest tests/test_expression_tokenizer_arrays.py -v
pytest tests/ -k "token" -v  # Run all tokenizer tests
```

## Estimated Time

15-20 minutes
